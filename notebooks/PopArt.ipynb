{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a30e949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "content_path = tf.keras.utils.get_file('photo-1563306406-e66174fa3787', \n",
    "                                       'https://images.unsplash.com/photo-1563306406-e66174fa3787')\n",
    "\n",
    "style_path = tf.keras.utils.get_file('Andy-Warhol--Marilyn-Monroe-Hot-Pink-1967-Andy-Warhol-Poster.jpg',\n",
    "                                     'https://product-image.juniqe-production.juniqe.com/media/catalog/product/seo-cache/x800/7/7/776-8-101P-13x18-1/Andy-Warhol--Marilyn-Monroe-Hot-Pink-1967-Andy-Warhol-Poster.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be7aea54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://drive.google.com/file/d/1Ip9KDzGiCeAH7hmeCC7CkWgA0GwkWCTG/view?usp=sharing\n",
      "  65536/Unknown - 0s 1us/step"
     ]
    }
   ],
   "source": [
    "content_path = tf.keras.utils.get_file('IMG_4830.HEIC.jpg','https://drive.google.com/file/d/1Ip9KDzGiCeAH7hmeCC7CkWgA0GwkWCTG/view?usp=sharing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "431a62a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_scaler(image, max_dim = 256):\n",
    "  # Casts a tensor to a new type.\n",
    "  original_shape = tf.cast(tf.shape(image)[:-1], tf.float32)\n",
    "  # Creates a scale constant for the image\n",
    "  scale_ratio = 4 * max_dim / max(original_shape)\n",
    "  # Casts a tensor to a new type.\n",
    "  new_shape = tf.cast(original_shape * scale_ratio, tf.int32)\n",
    "  # Resizes the image based on the scaling constant generated above\n",
    "  return tf.image.resize(image, new_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b8fd81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path_to_img):\n",
    "  # Reads and outputs the entire contents of the input filename.\n",
    "  img = tf.io.read_file(path_to_img)\n",
    "  # Detect whether an image is a BMP, GIF, JPEG, or PNG, and \n",
    "  # performs the appropriate operation to convert the input \n",
    "  # bytes string into a Tensor of type dtype\n",
    "  img = tf.image.decode_image(img, channels=3)\n",
    "  # Convert image to dtype, scaling (MinMax Normalization) its values if needed.\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "  # Scale the image using the custom function we created\n",
    "  img = img_scaler(img)\n",
    "  # Adds a fourth dimension to the Tensor because\n",
    "  # the model requires a 4-dimensional Tensor\n",
    "  return img[tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b44b9577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 15:30:53.950805: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Unknown image file format. One of JPEG, PNG, GIF, BMP required. [Op:DecodeImage]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jk/3fclt6xd4zv_bg16j7pwc95m0000gn/T/ipykernel_20432/2854521852.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcontent_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstyle_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/jk/3fclt6xd4zv_bg16j7pwc95m0000gn/T/ipykernel_20432/2660115444.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path_to_img)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m# performs the appropriate operation to convert the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m# bytes string into a Tensor of type dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0;31m# Convert image to dtype, scaling (MinMax Normalization) its values if needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_image_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/tensorflow/python/ops/image_ops_impl.py\u001b[0m in \u001b[0;36mdecode_image\u001b[0;34m(contents, channels, dtype, name, expand_animations)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             dtype=dtype), dest_dtype)\n\u001b[1;32m   3205\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m         return gen_image_ops.decode_image(\n\u001b[0m\u001b[1;32m   3207\u001b[0m             \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m             \u001b[0mchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/tensorflow/python/ops/gen_image_ops.py\u001b[0m in \u001b[0;36mdecode_image\u001b[0;34m(contents, channels, dtype, expand_animations, name)\u001b[0m\n\u001b[1;32m   1046\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6895\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6896\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6897\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6898\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Unknown image file format. One of JPEG, PNG, GIF, BMP required. [Op:DecodeImage]"
     ]
    }
   ],
   "source": [
    "content_image = load_img(content_path)\n",
    "style_image = load_img(style_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af54dff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(content_image[0])\n",
    "plt.title('Content Image')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(style_image[0])\n",
    "plt.title('Style Image')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99a7155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a pre-trained VGG model which takes an input and returns a list of intermediate output values \n",
    "def vgg_layers(layer_names):\n",
    "  \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n",
    "  # Load our model. Load pretrained VGG, trained on imagenet data\n",
    "  vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "  vgg.trainable = False\n",
    "  outputs = [vgg.get_layer(name).output for name in layer_names]\n",
    "  model = tf.keras.Model([vgg.input], outputs)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdfc121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(input_tensor):\n",
    "  # Tensor contraction over specified indices and outer product.\n",
    "  # Matrix multiplication\n",
    "  result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
    "  # Save the shape of the input tensor\n",
    "  input_shape = tf.shape(input_tensor)\n",
    "  # Casts a tensor to a new type.\n",
    "  num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
    "  # Divide matrix multiplication output to num_locations\n",
    "  return result/(num_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312ffc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use block5 conv2 layer for content \n",
    "content_layers = ['block5_conv2'] \n",
    "# We will use conv1 layers from every block for style \n",
    "style_layers = ['block1_conv1','block2_conv1','block3_conv1', 'block4_conv1','block5_conv1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93589919",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleContentModel(tf.keras.models.Model):\n",
    "  def __init__(self, style_layers, content_layers):\n",
    "    super(StyleContentModel, self).__init__()\n",
    "\n",
    "    # The main \n",
    "    self.vgg =  vgg_layers(style_layers + content_layers)\n",
    "    self.vgg.trainable = False\n",
    "\n",
    "    # Used as keys in dict creation\n",
    "    self.style_layers = style_layers\n",
    "    self.content_layers = content_layers\n",
    "    # self.num_style_layers = len(style_layers)\n",
    "    \n",
    "\n",
    "  def call(self, inputs):\n",
    "    # Process the image input\n",
    "    \"Expects float input in [0,1]\"\n",
    "    inputs = inputs*255.0\n",
    "    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n",
    "\n",
    "    # Feed the preprocessed image to the VGG19 model\n",
    "    outputs = self.vgg(preprocessed_input)\n",
    "    # Separate style and content outputs\n",
    "    style_outputs, content_outputs = (outputs[:len(self.style_layers)], \n",
    "                                      outputs[len(self.style_layers):])\n",
    "    # Process style output before dict creation\n",
    "    style_outputs = [gram_matrix(style_output)\n",
    "                     for style_output in style_outputs]\n",
    "\n",
    "\n",
    "    # Create two dicts for content and style outputs\n",
    "    content_dict = {content_name:value \n",
    "                    for content_name, value \n",
    "                    in zip(self.content_layers, content_outputs)}\n",
    "    style_dict = {style_name:value\n",
    "                  for style_name, value\n",
    "                  in zip(self.style_layers, style_outputs)}\n",
    "    \n",
    "    return {'content':content_dict, 'style':style_dict}\n",
    "  \n",
    "extractor = StyleContentModel(style_layers, content_layers)\n",
    "# Set your style and content target values:\n",
    "style_targets = extractor(style_image)['style']\n",
    "content_targets = extractor(content_image)['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007437b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an optimizer. The paper recommends LBFGS, but Adam works okay, too:\n",
    "opt = tf.optimizers.Adam(learning_rate=0.005, beta_1=0.99, epsilon=1e-1)\n",
    "\n",
    "# To optimize this, use a weighted combination of the two losses to get the total loss:\n",
    "style_weight=1e-2\n",
    "content_weight=1e4\n",
    "\n",
    "def style_content_loss(outputs):\n",
    "    style_outputs = outputs['style']\n",
    "    content_outputs = outputs['content']\n",
    "    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) \n",
    "                           for name in style_outputs.keys()])\n",
    "    style_loss *= style_weight / len(style_layers)\n",
    "\n",
    "    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) \n",
    "                             for name in content_outputs.keys()])\n",
    "    content_loss *= content_weight / len(content_layers)\n",
    "    loss = style_loss + content_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96d6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_variation_weight=500\n",
    "\n",
    "@tf.function()\n",
    "def train_step(image):\n",
    "  with tf.GradientTape() as tape:\n",
    "    outputs = extractor(image)\n",
    "    loss = style_content_loss(outputs)\n",
    "    loss += total_variation_weight*tf.image.total_variation(image)\n",
    "\n",
    "  grad = tape.gradient(loss, image)\n",
    "  opt.apply_gradients([(grad, image)])\n",
    "  image.assign(tf.clip_by_value(image, \n",
    "                                clip_value_min=0.0, \n",
    "                                clip_value_max=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c319d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as display\n",
    "\n",
    "image = tf.Variable(content_image)\n",
    "epochs = 20\n",
    "steps_per_epoch = 100\n",
    "step = 0\n",
    "for n in range(epochs):\n",
    "  for m in range(steps_per_epoch):\n",
    "    step += 1\n",
    "    train_step(image)\n",
    "    print(\".\", end='')\n",
    "  display.clear_output(wait=True)\n",
    "  plt.imshow(image[0])\n",
    "  plt.show()\n",
    "  print(\"Train step: {}\".format(step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80942b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  # 1 - Generate images\n",
    "  predictions = model(test_input, training=False)\n",
    "  # 2 - Plot the generated images\n",
    "  fig = plt.figure(figsize=(4,4))\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "  # 3 - Save the generated images\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67061c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f314773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.preprocessing.image.save_img('stylized-image.png', image[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
